# ğŸ§¬ ABI ETL Pipeline - Intelligence Biotech Data Warehouse

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Prefect](https://img.shields.io/badge/Prefect-2.14.21-purple.svg)](https://www.prefect.io/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ Vue d'ensemble

Pipeline ETL moderne orchestrÃ© par **Prefect** qui transforme les donnÃ©es d'intelligence biotechnologique en entrepÃ´t de donnÃ©es optimisÃ© pour l'analytique BI. ConÃ§u pour l'**ABI Platform** avec monitoring avancÃ©, retry automatique et validation qualitÃ© complÃ¨te.

## ğŸ—ï¸ Architecture

### Design Star Schema
Architecture **Star Schema** optimisÃ©e pour l'analyse BI avec les avantages suivants:

1. **Performance analytique** - RequÃªtes OLAP ultra-rapides
2. **SimplicitÃ©** - Structure intuitive : faits au centre, dimensions autour
3. **CompatibilitÃ© BI** - OptimisÃ© pour Power BI, Tableau, Superset
4. **ScalabilitÃ©** - Gestion de volumes massifs de donnÃ©es biotech
5. **Ã‰volutivitÃ©** - Ajout facile de nouvelles dimensions/faits

### Schema de DonnÃ©es
- **Tables de Faits** : Ã‰vÃ©nements mesurables (Ã©valuations technologiques, mÃ©triques financiÃ¨res, scores d'entreprises)
- **Tables de Dimensions** : Attributs descriptifs (technologies, publications, entreprises, dates)
- **ClÃ©s de Substitution** : Chaque dimension possÃ¨de une clÃ© synthÃ©tique pour performance et cohÃ©rence
- **Contraintes d'IntÃ©gritÃ©** : Foreign keys, unique constraints, validations mÃ©tier

### Orchestration Prefect
- **ğŸ”„ Retry automatique** - 2-3 tentatives configurables par tÃ¢che
- **â±ï¸ Timeout management** - DÃ©lais adaptÃ©s par Ã©tape ETL
- **ğŸ“Š Monitoring temps rÃ©el** - Interface web complÃ¨te
- **ğŸš¨ Alertes** - Notifications Ã©checs/succÃ¨s configurables
- **ğŸ“… Scheduling** - ExÃ©cution quotidienne automatique
- **ğŸ¯ Artifacts** - Rapports et mÃ©triques sauvegardÃ©s

## ğŸ“‚ Structure du Projet
```
bi-etl-pipeline/
â”œâ”€â”€ ğŸ“ config/                  # Fichiers de configuration
â”‚   â”œâ”€â”€ config.yaml            # Configuration principale
â”‚   â”œâ”€â”€ prefect_config.py      # Configuration Prefect
â”‚   â””â”€â”€ performance_config.py  # ParamÃ¨tres de performance
â”œâ”€â”€ ğŸ“ data/                   # RÃ©pertoires de donnÃ©es
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es sources
â”‚   â”œâ”€â”€ processed/             # DonnÃ©es transformÃ©es
â”‚   â”œâ”€â”€ archive/               # Archives
â”‚   â””â”€â”€ exports/               # Exports BI
â”œâ”€â”€ ğŸ“ scripts/                # Scripts ETL
â”‚   â”œâ”€â”€ extract.py             # Extraction de donnÃ©es
â”‚   â”œâ”€â”€ transform.py           # Transformation star schema
â”‚   â”œâ”€â”€ load.py                # Chargement PostgreSQL
â”‚   â””â”€â”€ setup_prefect.py       # Configuration Prefect
â”œâ”€â”€ ğŸ“ flows/                  # Flows Prefect
â”‚   â””â”€â”€ prefect_etl_flows.py   # Orchestration principale
â”œâ”€â”€ ğŸ“ models/                 # ModÃ¨les SQLAlchemy ORM
â”‚   â””â”€â”€ models.py              # SchÃ©ma de la base
â”œâ”€â”€ ğŸ“ validation/             # ContrÃ´les qualitÃ©
â”‚   â””â”€â”€ data_validator.py      # Validation multi-niveaux
â”œâ”€â”€ ğŸ“ logs/                   # Journaux d'exÃ©cution
â”œâ”€â”€ ğŸ“ tests/                  # Tests unitaires
â”‚   â””â”€â”€ test_etl.py            # Tests des composants ETL
â”œâ”€â”€ ğŸ“ docs/                   # Documentation
â”‚   â””â”€â”€ PREFECT_GUIDE.md       # Guide complet Prefect
â”œâ”€â”€ etl_pipeline.py            # Pipeline standard
â”œâ”€â”€ etl_pipeline_prefect.py    # Pipeline orchestrÃ© Prefect
â””â”€â”€ requirements.txt           # DÃ©pendances Python
```

## ğŸš€ DÃ©marrage Rapide

### 1. Installation de l'environnement
```bash
# Cloner le repository
git clone https://github.com/azyyzme01/ABI_ETL.git
cd bi-etl-pipeline

# CrÃ©er l'environnement virtuel
python -m venv mon_env
mon_env\Scripts\activate  # Windows
# source mon_env/bin/activate  # Linux/Mac

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 2. Configuration Base de DonnÃ©es
```bash
# CrÃ©er le fichier .env Ã  partir du template
copy .env.template .env  # Windows
# cp .env.template .env  # Linux/Mac

# Ã‰diter .env avec vos credentials
DB_HOST=localhost
DB_PORT=5432
DB_NAME=abi_warehouse
DB_USER=your_username
DB_PASSWORD=your_password
```

### 3. Initialisation PostgreSQL
```sql
-- CrÃ©er la base de donnÃ©es
CREATE DATABASE abi_warehouse;
CREATE USER etl_user WITH PASSWORD 'secure_password';
GRANT ALL PRIVILEGES ON DATABASE abi_warehouse TO etl_user;
```

### 4. Configuration Prefect
```bash
# Setup automatique de Prefect
python scripts/setup_prefect.py

# DÃ©marrer le serveur Prefect (terminal 1)
prefect server start

# DÃ©marrer un worker (terminal 2)
prefect worker start --pool default-agent-pool
```

### 5. ExÃ©cution du Pipeline
```bash
# Mode Prefect (recommandÃ©)
python etl_pipeline_prefect.py

# Mode standard (fallback)
python etl_pipeline_prefect.py --force-standard

# Avec options avancÃ©es
python etl_pipeline_prefect.py --config config/config.yaml --skip-validation
```

### 6. Interface de Monitoring
- **Prefect UI** : http://127.0.0.1:4200
- **Flows** â†’ `ABI ETL Pipeline` â†’ **Run** pour exÃ©cution manuelle

## ğŸ“Š Sources de DonnÃ©es

### ğŸ§ª DonnÃ©es Scientifiques
- **PrÃ©dictions TRL** (`realistic_trl_predictions.xlsx`)
  - Ã‰volution de la maturitÃ© technologique (Technology Readiness Level)
  - PrÃ©dictions temporelles par secteur biotechnologique
  - ModÃ¨les ML de progression TRL

- **Articles PubMed** (`pubmed_articles_2015_2025_mergeds.xlsx`)
  - Publications scientifiques 2015-2025
  - MÃ©tadonnÃ©es complÃ¨tes : auteurs, keywords, impact factor
  - Classification par domaine de recherche

### ğŸ¢ DonnÃ©es Entreprises
- **Entreprises Biotech** (`abi_targeted_biotech_dataset_2019_2025.csv`)
  - Informations financiÃ¨res et de marchÃ©
  - Scores stratÃ©giques et d'innovation
  - MÃ©triques de performance trimestrielles

## ğŸ¯ Cas d'Usage BI

### ğŸ“ˆ Analytics SupportÃ©s
1. **Suivi MaturitÃ© Technologique**
   - Heatmaps de progression TRL
   - Tendances par secteur biotech
   - PrÃ©dictions d'Ã©volution

2. **Analyse des Tendances de Recherche**
   - Patterns de publication par domaine
   - Analyse d'impact et citations
   - Identification d'innovations Ã©mergentes

3. **Ã‰valuation d'Entreprises**
   - Scoring stratÃ©gique multi-facteurs
   - Benchmarking sectoriel
   - Matrices de positionnement

4. **Analyse FinanciÃ¨re**
   - MÃ©triques de performance Q/Q
   - Ratios financiers sectoriels
   - DÃ©tection d'anomalies

5. **Intelligence d'Investissement**
   - Recommandations data-driven
   - Scoring de potentiel d'innovation
   - Analyse de risque sectoriel

## ï¿½ CaractÃ©ristiques Techniques

### âœ… Idempotence Garantie
Le pipeline est **entiÃ¨rement idempotent** - exÃ©cutions multiples avec les mÃªmes donnÃ©es produisent des rÃ©sultats identiques sans doublons grÃ¢ce aux techniques UPSERT PostgreSQL.

### ğŸ” Validation Multi-Niveaux
1. **Fichiers sources** - Existence, format, taille
2. **DonnÃ©es extraites** - ComplÃ©tude, types, cohÃ©rence
3. **DonnÃ©es transformÃ©es** - IntÃ©gritÃ© rÃ©fÃ©rentielle, schÃ©ma
4. **Base de donnÃ©es** - Contraintes, rÃ¨gles mÃ©tier
5. **RÃ¨gles business** - Validations sectorielles spÃ©cifiques

### ğŸ“Š MÃ©triques et Monitoring
- **DurÃ©e d'exÃ©cution** par Ã©tape ETL
- **Nombre d'enregistrements** traitÃ©s/rejetÃ©s
- **Taux de succÃ¨s/Ã©chec** historique
- **Utilisation ressources** systÃ¨me
- **QualitÃ© des donnÃ©es** en temps rÃ©el
- **Artifacts Prefect** - Rapports dÃ©taillÃ©s sauvegardÃ©s

### ğŸ›¡ï¸ Robustesse et FiabilitÃ©
- **Retry automatique** - Configuration par type d'erreur
- **Timeout management** - DÃ©lais adaptÃ©s par complexitÃ©
- **Error handling** - Classification et escalade
- **Logging structurÃ©** - TraÃ§abilitÃ© complÃ¨te
- **Rollback capabilities** - Restauration en cas d'Ã©chec

## ğŸ› ï¸ Maintenance et Operations

### ğŸ“‹ Commandes Utiles
```bash
# Tests complets
pytest tests/ -v --cov=scripts

# Validation qualitÃ© donnÃ©es
python validation/data_validator.py

# Reset environnement Prefect
prefect server database reset -y

# Monitoring logs
tail -f logs/etl_pipeline_$(date +%Y%m%d)*.log

# Export mÃ©triques
python scripts/export_metrics.py --format json
```

### ğŸ“ Fichiers de Logs
- **Logs d'exÃ©cution** : `logs/etl_pipeline_YYYYMMDD_HHMMSS.log`
- **MÃ©triques de performance** : `logs/etl_metrics_YYYYMMDD_HHMMSS.yaml`
- **Rapports de validation** : `logs/validation_report_YYYYMMDD_HHMMSS.yaml`
- **Logs Prefect** : Interface web + base de donnÃ©es intÃ©grÃ©e

### ğŸš¨ Alertes et Notifications
Configuration dans `config/prefect_config.py` :
- **Ã‰checs critiques** - Email + Slack
- **SuccÃ¨s avec warnings** - Log + Dashboard
- **MÃ©triques de performance** - Seuils configurables
- **QualitÃ© des donnÃ©es** - Alertes par seuil

## ğŸ“š Documentation

- **[Guide Prefect Complet](docs/PREFECT_GUIDE.md)** - Orchestration avancÃ©e
- **[Architecture Technique](docs/ARCHITECTURE.md)** - Design dÃ©taillÃ©
- **[Guide de DÃ©ploiement](docs/DEPLOYMENT.md)** - Production setup
- **[API Reference](docs/API.md)** - Documentation code

## ğŸ¤ Contribution

1. **Fork** le repository
2. **CrÃ©er** une branche feature (`git checkout -b feature/amÃ©lioration`)
3. **Commit** les changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. **Push** vers la branche (`git push origin feature/amÃ©lioration`)
5. **CrÃ©er** une Pull Request

## ğŸ“ License

Ce projet est sous licence **MIT** - voir [LICENSE](LICENSE) pour les dÃ©tails.

## ğŸ‘¥ Ã‰quipe

- **Data Engineering** - [@azyyzme01](https://github.com/azyyzme01)
- **Architecture** - ABI Platform Team
- **Analytics** - BI Team

---

**ğŸš€ DÃ©veloppÃ© avec â¤ï¸ pour l'intelligence biotechnologique**
